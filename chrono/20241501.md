### O que eu aprendi hoje? 

# Sockets

Envio de mensagens entre sockets utilizam frequentemente buffers em bytes para comunicação, muitas das implementações como a de Go quando lemos um pacote de forma continua até atingir EOF (considerado um erro) é porque a conexão foi fechada. 

# System call epoll

`epoll` é uma system call que permite async I/O de forma eficiente. Utilzando `epoll` é possível "observar" vários descritores de arquivos ao mesmo tempo (pipes, sockets, arquivos etc.) e enviar notificações baseada em alguma ação como estar pronto para escrita ou uma leitura pendente. Esse mecanismo é utilizado em modelos conhecidos como o event loop que utiliza a biblitoeca libuv que por sua vez utiliza o `epoll` quando rodado em sistemas Linux.

# Nível de acesso no kernel e processos

Existe uma diferenciação em sistemas Linux entre **user space** e **kernel space**, quando falamos de **user space** estamos falando de processos do usuário, ou seja tudo que não é do kernel. Esses processos tem um acesso limitado a memória, enquanto os processos do **kernel space** tem acesso a toda memória. Processos rodando à nível de usuário, **user space**, não tem acesso ao **kernel space** e sim apenas uma pequena perta da interface do kernel conhecida como system calls. Se um processo efetua uma system call como `epoll` o processo emite um chamada de interrupção pro kernel que irá processar e designar uma ação a um interrupt handler. 

Além da diferenciação entre espaço de kernel e de usuário há um nível de "permissionamento" chamado ring que varia de ring 0 à 3. 

Esse conceito funciona a nível de CPU e certas instruções de assembly só podem ser executadas a partir do kernel que roda em ring 0. A maioria dos processos roda em ring 3 tendo acesso apenas a um subconjunto de instruções diretamente. 

Supondo o seguinte cenário onde seria necessário desenvolver um software que monitore a temperatura de um componente como o CPU. Na arquitetura x86 da Intel esse tipo de instrução precisa rodar no ring 0, então seria necessário criar um módulo no kernel, **kernel module**, e um programa intermediário que se comunicaria com essa interface. 

# Forward proxy, reverse proxy e VPN

- Forward proxy: é centrada no usuário (client) está ligada ao tráfego outbound de rede, em vez da requisição ir diretamente pro servidor ela passa por um intermediário que redireciona pro servidor. Se eu estou utilizando um proxy e tento acessar google.com primeiro a requisição vai pro servidor do proxy que é encaminhado então pro servidor do Google.
- Reverse proxy: é centrado no servidor (server) está ligado ao tráfego inbound da rede, a requisição chega num intermediário que direciona a requisição pra um servidor normalmente dentro duma mesma rede. Nesse sentido o reverse proxy funciona como um load balancer balancenado a carga de um domínio. Por exemplo, se google.com tiver um reverse proxy na frente a requisição vai chegar no servidor do proxy e vai direcionar pra um dos vários servidores da Google para responser a requisição. 
- VPN: funciona de forma semelhante ao forward proxy com a diferença que proporciona mais funcionalidades principalmente focada na segurança através de protocolos de segurança e o que é chamado de tunneling. Uma VPN não apenas criptografa e assegura o conteúdo dentro de um pacote na rede, mas sim o pacote inteiro. Isso é possível pois maioria desses protocolos como OpenVPN criam uma interface de rede virtual utilizando tun/tap (no caso do Linux) que direciona os pacotes pra essa interface de rede que só existe virtualmente.

# Reactor e event loop

Reactor é um padrão utilizado em sistemas concorrentes para possibilitar o processo de requisições de forma eficiente. Há duas estruturas: reactor e handlers. Os eventos de I/O são recebidos pelo reactor que roda em uma thread separada e envia as tasks (requisições) para o handler adequado. Uma analogia seria uma atendentende que recebe as ligações e direciona para a pessoa correta. O handler é a implementação do que deve ser feito, é a ação. 

Esse padrão existe para conseguir atender níveis altos de concorrência como acima de 10.000 requisições concorrentes. No modelo clássico cliente-servidor cada requisição é tratada como uma thread separada assim que a requisição chega, o que causa um overhead principalmente relacionado ao context switch.

